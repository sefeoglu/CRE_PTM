# Continual Relation Extraction with Pretrained Large Language Models (CRE-PLM)
Continual Relation Extraction Utilizing Pretrained Large Language Model
## Method
![Method](https://github.com/sefeoglu/CRE_PTM/blob/master/doc/CRE_PLM.png)

## Folder Structure
```xml
.
├── LICENSE
├── README.md
├── config.ini
├── data
│   ├── fewrel
│   └── tacred
├── doc
├── requirements.txt
├── resulting_metrics
└── src
    ├── CRE
    ├── clean
    ├── data-preparetation
    ├── metrics
    ├── viz
    ├── utils.py
    └── zero_shot_prompting
````
        
## How works



## Results
### Seen Task Accuracy.

| **Dataset**      | **Method**                          | **1**   | **2**   | **3**   | **4**   | **5**   | **6**   | **7**   | **8**   | **9**   | **10**  |
|------------------|-------------------------------------|---------|---------|---------|---------|---------|---------|---------|---------|---------|---------|
| **TACRED**       | **EMAR**                            | 73.6    | 57.0    | 48.3    | 42.3    | 37.7    | 34.0    | 32.6    | 30.0    | 27.6    | 25.1    |
|                  | **EA-EMR**                          | 47.1    | 40.1    | 38.3    | 29.9    | 28.4    | 27.3    | 26.9    | 25.8    | 22.9    | 19.8    |
|                  | **CML**                             | 57.2    | 51.4    | 41.3    | 39.3    | 35.9    | 28.9    | 27.3    | 26.9    | 24.8    | 23.4    |
|                  | **EMAR-BERT**                       | 96.6    | 85.7    | 81.0    | 78.6    | 73.9    | 72.3    | 71.7    | 72.2    | 72.6    | 71.0    |
|                  | **RP-CRE**~\cite{cui-etal-2021-refining} | 97.6    | 90.6    | 86.1    | 82.4    | 79.8    | 77.2    | 75.1    | 73.7    | 72.4    | 72.4    |
|                  | **CRL**~\cite{zhao-etal-2022-consistent}  | 97.7    | 93.2    | 89.8    | 84.7    | 84.1    | 81.3    | 80.2    | 79.1    | 79.0    | 78.0    |
|                  | **KIP-Framework**~\cite{zhang_2022}  | `98.3`  | 95.0    | 90.8    | 87.5    | 85.3    | 84.3    | 82.1    | 80.2    | 79.6    | 78.6    |
|                  | **CREST**~\cite{Le_Nguyen_2024}     | 97.3    | 91.4    | 82.3    | 82.5    | 79.2    | 75.8    | 78.8    | 77.4    | 78.6    | 79.4    |
|                  | **Ours with Flan-T5**               | 96.05   | `96.23` | 95.72   | `96.03` | 95.65   | 95.41   | 96.05   | 96.00   | `96.26` | 95.76   |
|                  | **Ours with Mistral**              | 95.00   | 94.82   | `96.43` | 95.95   | `96.56` | `96.96` | `96.76` | `96.92` | 95.80   | `96.89` |
|                  | **Ours with Llama2**               | 55.48   | 54.69   | 43.82   | 43.43   | 51.40   | 70.98   | 61.05   | 72.57   | 73.55   | 69.55   |
| **FewRel**       | **EA-EMR**                          | 88.5    | 69.0    | 59.1    | 54.2    | 47.8    | 46.1    | 43.1    | 40.7    | 38.6    | 35.1    |
|                  | **EMAR**                            | 88.5    | 73.2    | 66.6    | 63.8    | 55.8    | 54.3    | 52.9    | 50.9    | 48.8    | 46.3    |
|                  | **CML**                             | 91.2    | 74.8    | 68.2    | 58.2    | 53.7    | 50.4    | 47.8    | 44.4    | 43.1    | 39.7    |
|                  | **EMAR-BERT**                       | 98.8    | 89.1    | 89.5    | 85.7    | 83.6    | 84.8    | 79.3    | 80.0    | 77.1    | 73.8    |
|                  | **RP-CRE**~\cite{cui-etal-2021-refining} | 97.9    | 92.7    | 91.6    | 89.2    | 88.4    | 86.8    | 85.1    | 84.1    | 82.2    | 81.5    |
|                  | **CRL**~\cite{zhao-etal-2022-consistent}  | 98.2    | 94.6    | 92.5    | 90.5    | 89.4    | 87.9    | 86.9    | 85.6    | 84.5    | 83.1    |
|                  | **KIP-Framework**~\cite{zhang_2022}  | 98.4    | 93.5    | 92.0    | 91.2    | 90.0    | 88.2    | 86.9    | 85.6    | 84.1    | 82.5    |
|                  | **CREST**~\cite{Le_Nguyen_2024}     | 98.7    | 93.6    | 93.8    | 92.3    | 91.0    | 89.9    | 87.6    | 86.7    | 86.0    | 84.8    |
|                  | **Ours with Flan-T5 Base**               | 97.32   | 94.0    | 93.3    | 90.0    | `92.6`  | 84.7    | 85.4    | 79.4    | 77.8    | 69.9    |


### Whole and Average Accuracy

| **Method**                       | **TACRED (w)** | **TACRED (a)** | **FewRel (w)** | **FewRel (a)** |
|-----------------------------------|----------------|----------------|----------------|----------------|
| **EMR**                           | 21.8           | 26.5           | 42.0           | 54.1           |
| **EA-EMR**                        | 23.0           | 30.0           | 49.0           | 61.2           |
| **EMAR**                          | 31.0           | 36.3           | 53.8           | 68.1           |
| **CML**                           | 43.7           | 45.3           | --             | --             |
| **KIP-Framework**~\cite{zhang_2022} | 91.1           | 91.6           | 96.3           | 96.6           |
| **Ours with Flan-T5 Base**             | `93.78`        | `95.94`        | 90.34          | 95.62          |
| **Ours with Mistral-Instruct-v2.0**             | `96.12`        | `96.12`        | --             | --             |
| **Ours with Llama2-7B-hf-chat**               | 69.47          | 58.44          | --             | --             |
