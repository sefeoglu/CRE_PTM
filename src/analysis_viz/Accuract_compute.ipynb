{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zL8rIIo4MeoC"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_json(path):\n",
        "    with open(path, 'r', encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "    return data"
      ],
      "metadata": {
        "id": "8F01W9AKNZT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_accuracy(test_truth_folder, prediction_folder, experiment_id, task_id):\n",
        "  data = []\n",
        "  for t in range(1, task_id+1):\n",
        "    input_path = f\"{test_truth_folder}/run_{0}/task{1}/test_1.json\".format(experiment_id, t)\n",
        "    # print(input_path)\n",
        "    task_data = read_json(input_path)\n",
        "    data.extend(task_data)\n",
        "  y_true = [item['relation'] for item in data]\n",
        "  if task_id ==1:\n",
        "    preds_files = f\"{prediction_folder}/model_{experiment_id}_extracted/task_{task_id}_current_task_pred.json\"\n",
        "  else:\n",
        "    preds_files = f\"{prediction_folder}/model_{experiment_id}/task_{task_id}_seen_task.json\"\n",
        "  preds = [item['clean'] for item in read_json(preds_files)]\n",
        "  acc = accuracy_score(y_true, preds)\n",
        "\n",
        "  return acc\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5AJdk4E9Mnbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  results = []\n",
        "  for run_id in range(1,6):\n",
        "    for task_id in range(1,11):\n",
        "      test_truth_folder = '.'\n",
        "      prediction_folder = '.'\n",
        "      acc = compute_accuracy(test_truth_folder, prediction_folder, run_id, task_id)\n",
        "      # print(acc)\n",
        "      result= {'run_id':run_id, 'task_id':task_id, 'acc':acc}\n",
        "      results.append(result)\n"
      ],
      "metadata": {
        "id": "MBSp4O-jOtPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = pd.DataFrame(results)"
      ],
      "metadata": {
        "id": "7OWH7AvzPxgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#m==0\n",
        "results_df.groupby('task_id').mean().T"
      ],
      "metadata": {
        "id": "9kxda-NkPz2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#m==5\n",
        "results_df.groupby('task_id').mean().T"
      ],
      "metadata": {
        "id": "lyycFkWrQ6BD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#m==10\n",
        "results_df.groupby('task_id').mean().T"
      ],
      "metadata": {
        "id": "T27qjyB19ykq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#m==15\n",
        "results_df.groupby('task_id').mean().T"
      ],
      "metadata": {
        "id": "EtTzR9SVRDND"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}