{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmQ6kNTcXhsF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import logging\n",
        "import sys\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.metrics import det_curve\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkobjfZdXWrk"
      },
      "outputs": [],
      "source": [
        "\n",
        "def read_json(path):\n",
        "    \"\"\" Read a json file from the given path.\"\"\"\n",
        "    with open(path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    return data\n",
        "\n",
        "def write_json(path, data):\n",
        "    \"\"\" Write a json file to the given path.\"\"\"\n",
        "    if not os.path.exists(os.path.dirname(path)):\n",
        "        os.makedirs(os.path.dirname(path))\n",
        "\n",
        "    with open(path, 'w', encoding=\"utf-8\") as f:\n",
        "        json.dump(data, f, indent=4, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mytjJio-X2js"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "def compute_acc(prediction, ground, gt_relations, task_id):\n",
        "  \"\"\"\n",
        "  Compute accuracy and handle multiclass data for DET curve.\n",
        "\n",
        "  Args:\n",
        "      prediction: Predicted labels.\n",
        "      ground: True labels.\n",
        "\n",
        "  Returns:\n",
        "      acc: Accuracy score.\n",
        "  \"\"\"\n",
        "  # Get unique labels from both prediction and ground truth to ensure all classes are included\n",
        "  labels = list(set([item['relation'] for item in gt_relations]))\n",
        "\n",
        "  labels = sorted(labels)\n",
        "  labels = [label for label in labels if label !='']\n",
        "\n",
        "  # acc = accuracy_score(ground, prediction)\n",
        "\n",
        "  prediction = [pred if pred != '' else \"undefined\" for pred in prediction]\n",
        "  ground = [g if g != '' else \"undefined\" for g in ground]\n",
        "  # prediction = [pred   for pred in prediction if pred != '']\n",
        "  # ground = [g  for g in ground if g != '']\n",
        "  # Create a confusion matrix\n",
        "  conf_matrix = confusion_matrix(ground, prediction, labels=labels)\n",
        "\n",
        "  x_labels = labels\n",
        "  y_labels = labels\n",
        "  # x_labels.append(\"undefined\")\n",
        "\n",
        "  # Create a heatmap\n",
        "  plt.figure(figsize=(14, 12))  # Slightly larger for better readability\n",
        "  sns.heatmap(conf_matrix,\n",
        "              annot=True,\n",
        "              fmt='d',\n",
        "              cmap='BuPu',\n",
        "              xticklabels=x_labels,\n",
        "              yticklabels=labels,\n",
        "              annot_kws={\"size\": 24})  # Increased font size for annotations\n",
        "  plt.xlabel('Predicted', fontsize=24, fontweight='bold')   # Larger font for axis labels\n",
        "  plt.ylabel('True', fontsize=24, fontweight='bold')\n",
        "  plt.title('Confusion Matrix Heatmap - Flan T5 \\nOver Incremental Learning', fontsize=28, fontweight='bold')   # More prominent title\n",
        "  plt.xticks(fontsize=24, rotation=30, fontweight='bold')  # Rotate x-axis tick labels by 45 degrees\n",
        "  plt.yticks(fontsize=24, rotation=30, fontweight='bold')  # Rotate y-axis tick labels by 45 degrees\n",
        "\n",
        "\n",
        "  # Adjust layout to prevent overlap\n",
        "  plt.tight_layout()\n",
        "\n",
        "  # Save as a high-resolution image\n",
        "  plt.savefig(f'./confusion_matrix_t5/t5_task1_over_til_confusion_matrix_task_{task_id}.pdf', dpi=300)\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  return 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erad1WxyWdM7"
      },
      "source": [
        "**FewRel**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQaqfyWx1Trf"
      },
      "outputs": [],
      "source": [
        "def get_gt(gt_base, task_id, run_id):\n",
        "  gts=[]\n",
        "  print(\"test\")\n",
        "  print(gt_base)\n",
        "  for i in range(1, task_id+1):\n",
        "    gt_path = f'{gt_base}/run_{run_id}/task{i}/test_1.json'\n",
        "    gt = read_json(gt_path)\n",
        "    gts.extend([item['relation'] for item in gt])\n",
        "  return gts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcTlIxx8WjE8"
      },
      "outputs": [],
      "source": [
        "def get_list_acc(gt_path_base, experiment_result_path, task_1_path, model=\"t5\"):\n",
        "  results = []\n",
        "  for run_id   in range(1, 6):\n",
        "    gt_path = f'{gt_path_base}/run_{run_id}/task1/test_1.json'\n",
        "    gt = read_json(gt_path)\n",
        "    gt_relations = [item['relation'] for item in gt]\n",
        "    size_task1 = len(gt_relations)\n",
        "    task_1 = read_json(f'{experiment_result_path}/KMmeans_CRE_fewrel_{run_id}/task_task1_current_task_pred.json')\n",
        "    if model=='t5':\n",
        "      results.append({\"run\":run_id, \"task\":1, \"acc\":task_1[0]['acc']})\n",
        "    else:\n",
        "      task_1_pred = [item['predict'] for item in task_1]\n",
        "      # Filter out None values from both lists before calculating accuracy, ensuring the lists remain aligned\n",
        "      # Zip the two lists to iterate through them in parallel\n",
        "      filtered_data = [(p, g) for p, g in zip(task_1_pred, gt_relations) if p is not None and g is not None]\n",
        "      # If filtered_data is empty, set task_1_pred_filtered and gt_relations_filtered to empty lists to avoid errors\n",
        "      if not filtered_data:\n",
        "          task_1_pred_filtered = []\n",
        "          gt_relations_filtered = []\n",
        "      else:\n",
        "          # Unzip the filtered data back into separate lists\n",
        "          task_1_pred_filtered, gt_relations_filtered = zip(*filtered_data)\n",
        "      acc = compute_acc(task_1_pred, gt_relations, gt, 1 )\n",
        "      results.append({\"run\":run_id, \"task\":1, \"acc\":acc})\n",
        "    for task_id in range(2, 11):\n",
        "      print(task_id)\n",
        "      pred_path = f'{task_1_path}/KMmeans_CRE_fewrel_{run_id}/task_{task_id}_seen_task.json'\n",
        "      pred_task_1 = read_json(pred_path)[:len(gt)]\n",
        "      pred_relations_task_1 = [item['predict'] for item in pred_task_1]\n",
        "      # Filter out None values from both lists before calculating accuracy, ensuring the lists remain aligned\n",
        "      # Zip the two lists to iterate through them in parallel\n",
        "      gt_relations = get_gt(gt_path_base, task_id, run_id)[:len(gt)]\n",
        "      filtered_data = [(p, g) for p, g in zip(pred_relations_task_1, gt_relations) if p is not None and g is not None]\n",
        "      # If filtered_data is empty, set pred_relations_task_1_filtered and gt_relations_filtered to empty lists to avoid errors\n",
        "      if not filtered_data:\n",
        "          pred_relations_task_1_filtered = []\n",
        "          gt_relations_filtered = []\n",
        "      else:\n",
        "          # Unzip the filtered data back into separate lists\n",
        "          pred_relations_task_1_filtered, gt_relations_filtered = zip(*filtered_data)\n",
        "      task_1_acc = compute_acc(pred_relations_task_1_filtered, gt_relations_filtered, gt, task_id)\n",
        "      results.append({\"run\":run_id, \"task\":task_id, \"acc\":task_1_acc})\n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WAQP38vWjoI"
      },
      "outputs": [],
      "source": [
        "t5_results = get_list_acc('t5_format/test', 't5', 't5', \"t5_\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0iOgE3mWWZj4"
      },
      "outputs": [],
      "source": [
        "\n",
        "t5_df = pd.DataFrame(t5_results)\n",
        "mean_acc_t5 = t5_df.groupby('task').mean()\n",
        "std_acc_t5 = t5_df.groupby('task').std()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86Vhyvhx4H-J"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NdDueSR5cKWK",
        "outputId": "55f8a513-2b8b-4983-ab40-1e8a573a55a4"
      },
      "outputs": [],
      "source": [
        "mistral_results = get_list_acc('/content/llama_format_data/test', '/content/mistral_fewrel_clean/m_10', '/content/mistral_fewrel_clean/m_10', 'mistral')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NX9hUroOdqm-"
      },
      "outputs": [],
      "source": [
        "\n",
        "mean_mistral_tacred = pd.DataFrame(mistral_results).groupby('task').mean()['acc']\n",
        "std_mistral_tacred = pd.DataFrame(mistral_results).groupby('task').std()['acc']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0Ufl9gNgtMa"
      },
      "outputs": [],
      "source": [
        "llama_results = get_list_acc('/content/llama_format_data/test', '/content/llama_seen_clean_mist_code', '/content/llama_seen_clean_mist_code', 'llama')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMtin10Vhrg6"
      },
      "outputs": [],
      "source": [
        "mean_llama_tacred = pd.DataFrame(llama_results).groupby('task').mean()['acc']\n",
        "std_llama_tacred = pd.DataFrame(llama_results).groupby('task').std()['acc']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 806
        },
        "id": "G1dcGkh1dINa",
        "outputId": "f04d618d-2336-4556-f316-660f34523865"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Set up figure size\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Generate example data\n",
        "x = [f'{i}' for i in range(1, 11)]  # X values\n",
        "mean_t5 = mean_acc_t5['acc']  # Line values for Flan T5 Base\n",
        "std_t5 = std_acc_t5['acc']  # Standard deviation for Flan T5 Base\n",
        "\n",
        "# Scale data to percentages\n",
        "\n",
        "\n",
        "# Plot the Flan T5 Base line\n",
        "plt.plot(x, mean_t5, label='Flan T5 Base', color='purple',  marker='o')\n",
        "\n",
        "# Plot other models\n",
        "plt.plot(x, mean_mistral_tacred, label='Mistral-7b-Instruct-v2.0', color='orange',  marker='o')\n",
        "plt.plot(x, mean_llama_tacred, label='Llama-2-7b-chat-hf', color='green',  marker='o')\n",
        "\n",
        "# Optionally add the standard deviation as shaded areas (commented out for now)\n",
        "plt.fill_between(x, np.array(mean_t5) - np.array(std_t5), np.array(mean_t5) + np.array(std_t5),\n",
        "                 color='purple', alpha=0.1)\n",
        "plt.fill_between(x, np.array(mean_mistral_tacred) - np.array(std_mistral_tacred),\n",
        "                 np.array(mean_mistral_tacred) + np.array(std_mistral_tacred),\n",
        "                 color='orange', alpha=0.1)\n",
        "plt.fill_between(x, np.array(mean_llama_tacred) - np.array(std_llama_tacred),\n",
        "                 np.array(mean_llama_tacred) + np.array(std_llama_tacred),\n",
        "                 color='green', alpha=0.05)\n",
        "\n",
        "# Customize the plot\n",
        "plt.title(\"Task 1 Test Accuracy \\n across Incremental Learning\", fontsize=26, fontweight='bold')\n",
        "plt.xlabel(\"Base Training Task Index\", fontsize=26, fontweight='bold')\n",
        "plt.ylabel(\"Mean Accuracy (%)\", fontsize=26, fontweight='bold')\n",
        "plt.xticks(fontsize=20, fontweight='bold')  # Rotate x-axis labels and increase font size\n",
        "plt.yticks(fontsize=20, fontweight='bold')  # Increase font size of y-axis labels\n",
        "plt.legend(fontsize=20)\n",
        "plt.grid(True)\n",
        "\n",
        "# Improve layout and save the figure\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"task1_accuracy_fewrel.pdf\", dpi=300)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
