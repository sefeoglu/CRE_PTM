{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmQ6kNTcXhsF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkobjfZdXWrk"
      },
      "outputs": [],
      "source": [
        "\n",
        "def read_json(path):\n",
        "    \"\"\" Read a json file from the given path.\"\"\"\n",
        "    with open(path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    return data\n",
        "\n",
        "def write_json(path, data):\n",
        "    \"\"\" Write a json file to the given path.\"\"\"\n",
        "    if not os.path.exists(os.path.dirname(path)):\n",
        "        os.makedirs(os.path.dirname(path))\n",
        "\n",
        "    with open(path, 'w', encoding=\"utf-8\") as f:\n",
        "        json.dump(data, f, indent=4, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mytjJio-X2js"
      },
      "outputs": [],
      "source": [
        "def compute_acc(prediction, ground):\n",
        "  acc = accuracy_score(prediction, ground)\n",
        "  return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erad1WxyWdM7"
      },
      "source": [
        "**TACRED**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQaqfyWx1Trf"
      },
      "outputs": [],
      "source": [
        "def get_gt(gt_base, task_id, run_id):\n",
        "  gts=[]\n",
        "  print(\"test\")\n",
        "  print(gt_base)\n",
        "  for i in range(1, task_id+1):\n",
        "    gt_path = f'{gt_base}/run_{run_id}/task{i}/test_1.json'\n",
        "    gt = read_json(gt_path)\n",
        "    gts.extend([item['relation'] for item in gt])\n",
        "  return gts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcTlIxx8WjE8"
      },
      "outputs": [],
      "source": [
        "def get_list_acc(gt_path_base, experiment_result_path, task_1_path, model=\"t5\"):\n",
        "  results = []\n",
        "  for run_id   in range(1,6):\n",
        "    gt_path = f'{gt_path_base}/run_{run_id}/task1/test_1.json'\n",
        "    gt = read_json(gt_path)\n",
        "    gt_relations = [item['relation'] for item in gt] # Extract relation labels from ground truth\n",
        "    size_task1 = len(gt_relations)\n",
        "    task_1 = read_json(f'{experiment_result_path}/model{run_id}/task_task1_current_task_pred.json')\n",
        "    if model=='t5':\n",
        "      results.append({\"run\":run_id, \"task\":1, \"acc\":task_1[0]['acc']})\n",
        "    else:\n",
        "      task_1_pred = [item['relation'] for item in task_1]\n",
        "      # The problem was here: accuracy_score expects two lists of labels, not a list of dictionaries and a list of labels\n",
        "      # acc = accuracy_score(gt, task_1_pred)  # This line was causing the error\n",
        "      # Instead, use the extracted relation labels from ground truth (gt_relations)\n",
        "      acc = accuracy_score(gt_relations, task_1_pred)  # Corrected line\n",
        "      results.append({\"run\":run_id, \"task\":1, \"acc\":acc})\n",
        "      print(acc)\n",
        "    for task_id in range(2, 11):\n",
        "      print(task_id)\n",
        "      pred_path = f'{task_1_path}/model{run_id}/task_{task_id}_seen_task.json'\n",
        "      pred_task_1 = read_json(pred_path)[:len(gt)]\n",
        "      pred_relations_task_1 = [item['relation'] for item in pred_task_1]\n",
        "      # Filter out None values from both lists before calculating accuracy, ensuring the lists remain aligned\n",
        "      # Zip the two lists to iterate through them in parallel\n",
        "      gt_relations = get_gt(gt_path_base, task_id, run_id)\n",
        "      filtered_data = [(p, g) for p, g in zip(pred_relations_task_1, gt_relations) if p is not None and g is not None]\n",
        "      # If filtered_data is empty, set pred_relations_task_1_filtered and gt_relations_filtered to empty lists to avoid errors\n",
        "      if not filtered_data:\n",
        "          pred_relations_task_1_filtered = []\n",
        "          gt_relations_filtered = []\n",
        "      else:\n",
        "          # Unzip the filtered data back into separate lists\n",
        "          pred_relations_task_1_filtered, gt_relations_filtered = zip(*filtered_data)\n",
        "      task_1_acc = compute_acc(pred_relations_task_1_filtered, gt_relations_filtered)\n",
        "      results.append({\"run\":run_id, \"task\":task_id, \"acc\":task_1_acc})\n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WAQP38vWjoI"
      },
      "outputs": [],
      "source": [
        "t5_results = get_list_acc('task_data/test', '/content/t5', 't5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0iOgE3mWWZj4"
      },
      "outputs": [],
      "source": [
        "\n",
        "t5_df = pd.DataFrame(t5_results)\n",
        "mean_acc_t5 = t5_df.groupby('task').mean()\n",
        "std_acc_t5 = t5_df.groupby('task').std()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdDueSR5cKWK"
      },
      "outputs": [],
      "source": [
        "mistral_results = get_list_acc('/content/llama_format_data/test', '/content/mistral', '/content/mistral', 'mistral')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NX9hUroOdqm-"
      },
      "outputs": [],
      "source": [
        "mean_mistral_tacred = pd.DataFrame(mistral_results).groupby('task').mean()['acc']\n",
        "std_mistral_tacred = pd.DataFrame(mistral_results).groupby('task').std()['acc']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0Ufl9gNgtMa",
        "outputId": "a424ee92-c721-4a88-802a-a98ae3937671"
      },
      "outputs": [],
      "source": [
        "llama_results = get_list_acc('/content/llama_format_data/test', '/content/llama', '/content/llama', 'llama')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMtin10Vhrg6"
      },
      "outputs": [],
      "source": [
        "mean_llama_tacred = pd.DataFrame(llama_results).groupby('task').mean()['acc']\n",
        "std_llama_tacred = pd.DataFrame(llama_results).groupby('task').std()['acc']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 806
        },
        "id": "G1dcGkh1dINa",
        "outputId": "a7ac8023-ed63-4315-abbf-0915a8ffb784"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set up figure size\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Generate example data\n",
        "x = [f'{i}' for i in range(1, 11)]  # X values\n",
        "mean_t5 = mean_acc_t5['acc']  # Line values for Flan T5 Base\n",
        "std_t5 = std_acc_t5['acc']  # Standard deviation for Flan T5 Base\n",
        "\n",
        "# Scale data to percentages\n",
        "\n",
        "\n",
        "# Plot the Flan T5 Base line\n",
        "plt.plot(x, mean_t5, label='Flan T5 Base', color='purple',  marker='o')\n",
        "\n",
        "# Plot other models\n",
        "plt.plot(x, mean_mistral_tacred, label='Mistral-7b-Instruct-v2.0', color='orange',  marker='o')\n",
        "plt.plot(x, mean_llama_tacred, label='Llama-2-7b-chat-hf', color='green',  marker='o')\n",
        "\n",
        "# Optionally add the standard deviation as shaded areas (commented out for now)\n",
        "plt.fill_between(x, np.array(mean_t5) - np.array(std_t5), np.array(mean_t5) + np.array(std_t5),\n",
        "                 color='purple', alpha=0.1)\n",
        "plt.fill_between(x, np.array(mean_mistral_tacred) - np.array(std_mistral_tacred),\n",
        "                 np.array(mean_mistral_tacred) + np.array(std_mistral_tacred),\n",
        "                 color='orange', alpha=0.1)\n",
        "plt.fill_between(x, np.array(mean_llama_tacred) - np.array(std_llama_tacred),\n",
        "                 np.array(mean_llama_tacred) + np.array(std_llama_tacred),\n",
        "                 color='green', alpha=0.05)\n",
        "\n",
        "# Customize the plot\n",
        "plt.title(\"Task 1 Test Accuracy \\n across Incremental Learning\", fontsize=26, fontweight='bold')\n",
        "plt.xlabel(\"Base Training Task Index\", fontsize=26, fontweight='bold')\n",
        "plt.ylabel(\"Mean Accuracy (%)\", fontsize=26, fontweight='bold')\n",
        "plt.xticks(fontsize=20, fontweight='bold')  # Rotate x-axis labels and increase font size\n",
        "plt.yticks(fontsize=20, fontweight='bold')  # Increase font size of y-axis labels\n",
        "plt.legend(fontsize=22)\n",
        "plt.grid(True)\n",
        "\n",
        "# Improve layout and save the figure\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"task1_accuracy_tacred.pdf\", dpi=300)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
