{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgK_NT1779oM"
      },
      "outputs": [],
      "source": [
        "# !unzip relations.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpFjE4SEBUDD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cO5NQMhAc-7"
      },
      "outputs": [],
      "source": [
        "def read_json(path):\n",
        "    with open(path, 'r', encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "    return data\n",
        "\n",
        "def write_json(data, path):\n",
        "    if not os.path.exists(os.path.dirname(path)):\n",
        "        os.makedirs(os.path.dirname(path))\n",
        "    with open(path, 'w', encoding=\"utf-8\") as f:\n",
        "        json.dump(data, f, indent=4, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFgqD3LN-Uoo"
      },
      "outputs": [],
      "source": [
        "def calculate_false(test_folder, prediction_file_path, run_id, task_id):\n",
        "\n",
        "  test_relations = []\n",
        "  for t in range(1, task_id+1):\n",
        "    input_path = f\"{test_folder}run_{0}/task{1}/test_1.json\".format(run_id, t)\n",
        "    # print(input_path)\n",
        "    task_data = read_json(input_path)\n",
        "    test_data = [item['relation'] for item in task_data]\n",
        "    test_relations.extend(test_data)\n",
        "  predictions = read_json(prediction_file_path)\n",
        "\n",
        "  prediction_relations = [item['predict'] for item in predictions]\n",
        "  print(prediction_relations)\n",
        "  print(test_relations)\n",
        "  false_count = 0\n",
        "  for i in range(len(test_relations)):\n",
        "    if prediction_relations[i] != test_relations[i]:\n",
        "      false_count += 1\n",
        "  return false_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3dEVlwo-aST"
      },
      "outputs": [],
      "source": [
        "def calculate_non_defined(relations, prediction_file_path, run_id, task_id):\n",
        "  predictions = read_json(prediction_file_path)\n",
        "  prediction_relations = [item['predict'] for item in predictions]\n",
        "  relation_types_file = f\"{relations}/run_{run_id}/task{task_id}.json\"\n",
        "  relations = read_json(relation_types_file)\n",
        "\n",
        "  non_defined_count = 0\n",
        "  for relation in prediction_relations:\n",
        "    if relation not in relations:\n",
        "      non_defined_count += 1\n",
        "  return non_defined_count\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iaGO3WL-gY8"
      },
      "outputs": [],
      "source": [
        "def main(input_folder, test_folder, relations_folder):\n",
        "  results = []\n",
        "  for run_id in range(1, 6):\n",
        "    for task_id in range(1, 11):\n",
        "      print(f\"run_id: {run_id}, task_id: {task_id}\")\n",
        "      if task_id == 1:\n",
        "        prediction_file_path = f\"{input_folder}_{run_id}_extracted/task_task{task_id}_current_task_pred.json\"\n",
        "      else:\n",
        "        prediction_file_path = f\"{input_folder}_{run_id}_extracted/task_{task_id}_seen_task.json\"\n",
        "      non_defined = calculate_non_defined(relations_folder, prediction_file_path, run_id, task_id)\n",
        "      sum_false = calculate_false(test_folder, prediction_file_path, run_id, task_id)\n",
        "      error = {'run_id': run_id, 'task_id': task_id, 'non_defined': non_defined, 'sum_false':int(sum_false)}\n",
        "      results.append(error)\n",
        "\n",
        "  return results\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H473JNiB_MAo"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "  input_folder = \"./m_10/KMmeans_CRE_tacred\"\n",
        "  test_folder = \"./llama_format_data/test/\"\n",
        "  relations_folder = \"./relations/\"\n",
        "  results = main(input_folder, test_folder, relations_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0o8YdMYEnqZ"
      },
      "outputs": [],
      "source": [
        "write_json(results, \"./content/t5_fewrel_false_analysis.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsR8eSjQLu6i"
      },
      "outputs": [],
      "source": [
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "# Group by task_id and compute the mean of non_defined and sum of sum_false\n",
        "llama_result_df = df.groupby('task_id').agg(mean_non_defined=('non_defined', 'mean'),\n",
        "                                   sum_sum_false=('sum_false', 'mean')).reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yB3QrmOqRH4b"
      },
      "outputs": [],
      "source": [
        "# Llama2-7B\n",
        "print(llama_result_df.T.to_latex())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVykBsqIQhty"
      },
      "outputs": [],
      "source": [
        "# t5\n",
        "# print(t5_result_df.to_latex())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-u_1SLTHL798"
      },
      "outputs": [],
      "source": [
        "# mistral\n",
        "# print(mistral_result_df.to_latex())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jT7OHr5opi3K"
      },
      "outputs": [],
      "source": [
        "!zip -r tacred_error.zip ./content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSD3OPmxgmDb"
      },
      "outputs": [],
      "source": [
        "data_stats = []\n",
        "for run_id in range(1,6):\n",
        "  for task_id in range(1,11):\n",
        "    test_relations = []\n",
        "    for t in range(1, task_id+1):\n",
        "        input_path = f\"{test_folder}/run_{0}/task{1}/test_1.json\".format(run_id, t)\n",
        "        # print(input_path)\n",
        "        task_data = read_json(input_path)\n",
        "        test_data = [item['relation'] for item in task_data]\n",
        "        test_relations.extend(test_data)\n",
        "    print(f\"Count: {len(test_relations)}\")\n",
        "    stat = {'run_id': run_id, 'task_id': task_id, 'count': len(test_relations)}\n",
        "    data_stats.append(stat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NILGTRXrhE90"
      },
      "outputs": [],
      "source": [
        "data_stats_df = pd.DataFrame(data_stats)\n",
        "\n",
        "# Group by task_id and compute the mean of non_defined and sum of sum_false\n",
        "mean_data_stats_df = data_stats_df.groupby('task_id').agg(mean_non_defined=('count', 'mean')).reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hk4C7rNMhNbe"
      },
      "outputs": [],
      "source": [
        "print(mean_data_stats_df.T.to_latex())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
